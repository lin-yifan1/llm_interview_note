# 注意力机制

注意力机制可以简单理解为一个查询过程：

1. Query：主动发起查询的需求（“我想找什么？”）。
2. Key：被查询的索引或标签（“根据什么条件匹配？”）。
3. Value：最终需要获取的信息（“实际内容是什么？”）。

上述查询过程可以写为

$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}
$$

其中 $d_k$ 表示 Q 矩阵的列数。

在实际应用中，QKV 矩阵一般是通过三种输入（$X_{\text{query}}$，$X_{\text{key}}$，$X_{\text{value}}$）分别与三种权重矩阵（$W_Q$，$W_K$，$W_V$）相乘得来。根据注意力机制种类的不同，相应的输入也会不同。我们先假设输入统一为 $X$ 来解释注意力机制的核心原理，然后逐一介绍 Transformer 架构中的三种注意力机制：Self-Attention、Masked-Attention 和 Cross-Attention，最后介绍多头注意力（MHA）机制。

## 核心原理

为了解释注意力机制的核心原理，我们先假设 QKV 矩阵均由输入矩阵 X 分别乘以 $W_q$，$W_k$，$W_v$ 得来，X 的每一行对应一个词的嵌入向量（embedding vector）。

![](image/Attention-qkv.png)

如上图所示，注意力模块会首先计算一句话中每个词相对某个特定词（其嵌入向量用 $\underline{x}$ 表示）的权重：

$$
\alpha = q*K^T = \underline{x}W_q * (XW_k)^T
$$

> 在理解注意力机制的核心原理时，我们可以暂时忽略变换矩阵 $W_q$ 和 $W_k$，因此上式可以简化为 $\underline{x}*X^T$，最后得到的结果就是 $\underline{x}$ 与 X 中每一行的点积（dot-product）组成的向量，我们将其作为。

为了将权重归一化（使得权重向量元素之和为 1），我们还需要对权重除以 $\sqrt{d_k}$ 后取 softmax：

$$
\text{softmax}(\frac{\underline{x}W_q * (XW_k)^T}{\sqrt{d_k}})
$$

有了归一化的权重之后，我们就可以将其乘以 V 矩阵进行加权，其本质是对 X 的所有嵌入向量进行加权求和：

$$
\text{softmax}(\frac{\underline{x}W_q * (XW_k)^T}{\sqrt{d_k}})*XW_v
$$

这种计算注意力的方法我们称之为 scaled dot-product attention。

## Transformer 架构中的三种注意力机制

![](image/Transformer,_full_architecture.png)

在 Transformer 架构会用到三种不同的注意力机制，这三种机制的不同点主要在于输入的区别。

### Encoder 中的 Self-Attention

Self-Attention 的输入是源序列（比方说你问 ChatGPT 的问题），QKV 矩阵均由此输入生成，因此被称作自注意力。

### Decoder 中的 Masked Self-Attention

我们首先明确在训练和推理场景下 Decoder 行为的差异：

- 训练阶段：Decoder 的输入是完整的目标序列（例如翻译后的句子），但通过掩码机制（Masked Self-Attention）屏蔽未来 token，强制模型仅依赖当前位置之前的 token 进行预测（模拟自回归，autoregression）。
- 推理阶段：Decoder 无法看到完整目标序列，必须通过循环的“生成→拼接→再生成”的方式逐步生成序列（即自回归）。

训练阶段掩码机制的原理是只允许每个 token 和自己之前的 token 计算注意力，所以要加上一个左上角都是负无穷的矩阵 M：

$$
\text{MaskedAttention}(Q, K, V) = \text{softmax}\left(M + \frac{QK^\mathrm{T}}{\sqrt{d_k}}\right)V
$$

$$
M = \begin{bmatrix}
0 & -\infty & -\infty & \dots  & -\infty \\
0 & 0 & -\infty & \dots  & -\infty \\
0 & 0 & 0 & \dots  & -\infty \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots  & 0
\end{bmatrix}
$$

### Decoder 中的 Cross-Attention

Cross-Attention 采用 Encoder 的输出来生成 KV 矩阵，并使用 Self-Attention 的输出来生成 Q 矩阵。

## 多头注意力（MHA）

